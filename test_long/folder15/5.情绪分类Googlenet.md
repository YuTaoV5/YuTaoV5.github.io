---
headingNumber: true
---
# 5.情绪分类Googlenet


|||
| -------- | -------- | 
| $论文名称$ | CNN based efficient approach for emotion recognition |
| $作者$ | Muzaffer Aslan |
| $机构$ | Electrical-Electronics Engineering Department, Bingol University |
| $期刊$ | Journal of King Saud University Computer and Information Sciences 8.839/Q1 |
| $类别$ | 情绪分类 |
| $方法$ | 采用连续小波变换(CWT)将脑电信号转换为脑电信号图像(尺度图)，该方法对脑电信号的时频变化更为敏感。然后，利用预先训练好的GoogLeNet对EEG图像进行特征提取。最后，将获得的深度特征应用于流行的机器学习方法，如k-近邻(k-NN)、支持向量机(SVM)和极限学习机(ELM)分类器进行情感分类。 |
| $结论$ | 本研究将“积极”和“消极”两种情绪进行分类。综合实验结果表明，该方法在SVM、k-NN和ELM分类器中分别达到了98.78%、98.53%和98.41%的情绪检测准确率。 |
| $评价$ |文章最后比较模型时，我没有看到Googlenet最后softmax输出如何作为特征进行分类，这一点不太明白|
---

## 解决的问题
确定适合时频变化的窗口、选择分解级别和选择小波参数是具有挑战性的，因此，有必要开发一种自动情感识别方法。
## 创新点
- 利用CWT将脑电信号转换为脑电信号图像(标量图)
- 用GoogLeNet进行深度特征提取
## 主要工作
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131003145.png#pic_center%20=400x)

### 实验与数据集
| ||
|-|-|
|名称|GAMEEMO数据集|
|被试者|包含28名年龄在20-27岁之间的人|
|BCI设备|便携式可穿戴14通道EMOTIV EPOC +移动EEG设备|
|游戏时间|每个参与者在每个游戏中有5分钟的时间，总共有20分钟|
|信号的采样频率|128Hz|
|带通滤波|0.16Hz~43Hz|

数据集中包含的原始脑电图数据根据正在玩的四种不同电脑游戏的时间点进行评估，然后将其分类为
- “无聊”(G1)
- “平静”(G2)
- “恐怖”(G3)
- “有趣”(G4)。

#### CWT脑电信号图像

|||
|-|-|
|$x$|输入信号|
|$\psi$| 复共轭主小波|
|$a$| 频率的比例因子|
|$\tau$| 平移因子|
|$\frac{1}{abs(a)^2}$|描述不同尺度下的能量归一化|


$$\psi_{a, \tau}(t)>=\frac{1}{|a|^2} \int x(t) \psi^*\left(\frac{t-\tau}{a}\right) d t$$
$$\psi_{a, \tau}(t)=\frac{1}{|a|^2} \psi\left(\frac{t-\tau}{a}\right), a \neq 0$$
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131000913.png#pic_center%20=400x)

根据比例因子将灰度图像中像素亮度和对比度值的变化在128色图上进行着色然后resize为224*224大小的图像
### 特征提取

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131002302.png#pic_center%20=400x)


#### GoogLeNet模型

- 在当前的研究中使用了预先训练好的GoogLeNet模型。在当前的研究中，GoogLeNet基于训练集进行了微调。深度特征来自预先训练的GoogLeNet全连接层。
- GoogLeNet架构比其他CNN模型具有更多的层数，通过增加层数来减少参数的数量和计算成本
- GoogLeNet可以对同一条目进行多层次的特征提取。这些性质由不同尺度卷积层（1\*1,3\*3, 5\*5）决定的。

此外，通过在最后一层使用全局平均池化来降低连接密度，而不是使用全连接层。由于这些参数调优，参数的数量显著减少。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230130234119.png#pic_center%20=400x)
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230130234305.png#pic_center%20=400x)
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230130234821.png#pic_center =400x)

### 特征分类

由于K-NN没有训练过程，可以根据需要添加新的数据，并且具有简单的应用结构，因此在目前的研究中首选K-NN。因此，本研究采用k-NN模型，k-NN模型的值是经验确定的。通过实验评价，选取欧氏距离，k = 5，距离权重相等来评价邻域关系。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131001054.png#pic_center%20=400x)
正确分类的积极情绪符号化为真阳性(TP)，错误分类的积极情绪符号化为假阳性(FP)，正确分类的负面情绪符号化为真阴性(TN)，错误分类的负面情绪符号化为假阴性(FN)
$$Accuracy =\frac{T P+T N}{T P+T N+F P+F N} \times 100$$
$$Sensitivity =\frac{T P}{T P+F N} \times 100$$
$$Specificity =\frac{T N}{T N+F P} \times 100$$
$$Precision =\frac{T P}{T P+F P} \times 100$$
$$F1score =2 \times \frac{\text { Precision } \times \text { Sensitivity }}{\text { Precision }+\text { Sensitivity }} \times 100$$
### 训练
|||
|-|-|
|数据划分|训练(80%)、验证(10%)和测试(10%)|
|优化器|动量随机梯度下降(SGDM)算法|
|训练策略|在网络训练中，使用Dropout防止过度拟合|
|输出|最后的softmax层用于输入向量中数字集的概率分布，它根据softmax层中的概率分布来定义情绪是积极的还是消极的|
|初始学习率|0.001|
|验证频率|50|
|最小batch|16|
|最大epoch|32|
|耗时|26分38秒|

#### 硬件
在MATLAB (2019b)上进行的，MATLAB安装了四核Intel i7处理器，NVIDIA GTX 850 M GPU和16 GB内存的计算机。另外，GoogLeNet模块采用MATLAB深度学习工具箱。




## 结果

### 分类精度
当训练和验证损失值约减少到%0.5时，训练和验证的准确率达到92.02%
### 消融实验
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131002836.png#pic_center%20=400x)
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131002928.png#pic_center%20=400x)

### 模型对比
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230131002954.png#pic_center%20=400x)

---

$$C_{\psi }=\int \frac{|\widehat{\psi}(w)|^2}{|w|} d w<\infty$$
$$\psi_{a, \tau}(t)=\frac{1}{|a|^2} \psi\left(\frac{t-\tau}{a}\right), a \neq 0$$
$$y^m=\sum_i y_i^{m-1} * w_{i j}^m+c_i^m$$
$$n_c=\frac{1}{l} \sum_{i=1}^l x_i$$
$$v_c=\frac{1}{l} \sum_{i=1}^l\left(x_i-n_c\right)^2$$
$$\widehat{x}_i=\frac{x_i-n_c}{\sqrt{v_c^2+\varepsilon}}$$
$$y_i^m=d \widehat{x}_i+s$$
$$r_i^m=\max \left(0, y_i^m\right)$$
$$p_i^m=maximumoraverage \left\{r_i^m\right\}$$
$$\left\{\begin{array}{c}\min \frac{\|w\|^2}{2} \\ y_i\left(w^T x_i+b\right) \geq 1 i_{1,2}, \cdots, M\end{array}\right.$$
$$\min f(w, \zeta)=\frac{1}{2}\|w\|^2+C \sum_{i=1}^M \zeta_i$$
$$o_j=\sum_{i=1}^M \beta_i f\left(w_i, x_j+b_i\right), j=1,2, \cdots, N$$
$$o_j=\sum_{i=1}^M \beta_i f\left(w_i, x_j+b_i\right), j=1,2, \cdots, N$$
$$y_j=\sum_{i=1}^M \beta_i f\left(w_i, x_j+b_i\right), j=1,2, \cdots, N$$
$$\sum_{J=1}^M\left\|o_j-y_j\right\|=0$$
$$Y=H \beta$$
$$H=\left(\begin{array}{c}f\left(w_1 ; x_1, b_1\right) \cdots f\left(w_1 ; x_N, b_N\right) \\ \vdots \\ f\left(w_M ; x_1, b_1\right) \cdots f\left(w_M ; x_N, b_N\right)\end{array}\right)$$
$$L(X, Y ; \beta)=Y-H \beta^2$$
$$\hat{\beta}=H^{\dagger} Y$$
$$Accuracy =\frac{T P+T N}{T P+T N+F P+F N} \times 100$$
$$Sensitivity =\frac{T P}{T P+F N} \times 100$$
$$Specificity =\frac{T N}{T N+F P} \times 100$$
$$Precision =\frac{T P}{T P+F P} \times 100$$
$$F1score =2 \times \frac{\text { Precision } \times \text { Sensitivity }}{\text { Precision }+\text { Sensitivity }} \times 100$$