# 4.情绪分类Transformer
| 论文名称 |A Transformer based neural network for emotion recognition and visualizations of crucial EEG channels|
| -------- | -------- | 
| 期刊 |Physica A 3.778/Q1|
| 方法 |在这项研究中，我们提出了一种新的神经网络模型(DCoT)，深度卷积和变压器编码器用于基于EEG的情绪识别，通过探索情绪识别对每个EEG通道的依赖关系，并将捕获的特征可视化。然后，我们在基准数据集SEED上进行主题依赖和主题独立实验，该数据集包含积极、中性和消极情绪的脑电图数据。|
| 结论 |在学科依赖实验中，三个分类任务的平均准确率为93.83%。在非主题实验中，三个分类任务的平均准确率为83.03%。此外，我们还利用DCoT模型评估了每个EEG通道在情绪活动中的重要性，并将其可视化为脑图。此外，通过选取FT7、T7、TP7、P3、FC6、FT8、T8和F8这8个关键脑电通道，在2个分类任务和3个分类任务中均获得了令人满意的结果。使用少量的EEG通道进行情绪识别可以降低设备成本和计算成本，适合实际应用。|
| 评价 |有DW-CONV层的模型在每个频段上的性能都比没有DW-CONV层的模型更好，这说明了引入DW-CONV层的效率|


本文的主要贡献如下:
- 本文提出了一种新的基于eeg的情绪识别任务模型DCoT。卷积结构和Transformer编码器的应用使得该模型既能提取不同脑电信号通道的局部特征，又能提取情感识别的全局依赖关系。实验结果表明，我们的DCoT模型在识别精度方面优于之前的研究。

- 采用深度卷积融合脑电信号的多频域信息。这使得DCoT模型在保持EEG通道独立性的同时，增强了特征提取能力。

- 模型的学习过程更具有可解释性，因为学习过程是对EEG各通道在情绪识别中的重要性的探索。我们可以将DCoT模型提取的特征可视化为脑图，显示情绪活动中大脑的活跃区域。这是相对于其他现有方法的优势。

## 实验数据集
我们在以下实验中验证了基于SJTU情感EEG数据集(SEED)的方法的有效性。
15名受试者(男7名，女8名，年龄23.27±2.37岁)参加情绪实验收集脑电图数据，通过观看15段情绪刺激电影片段(每段约4分钟)捕捉每位受试者的高质量脑电图信号。这些电影片段被选择来刺激积极情绪、中性情绪和消极情绪，每个电影片段有5个对应的电影片段。在观看完每个电影片段(试验)后，这些参与者被要求通过回答问卷来记录他们对电影片段的情绪反应。每个受试者每两周进行一次实验，共进行三次。因此，每个受试者每节试验15次，每个受试者共3节。

## 主要工作

### 数据处理
- 将原始EEG信号(1000 Hz)降采样至200 Hz;
- 采用独立分量相关算法(ICA)去除EOG信号和闪烁伪影干扰。
- 利用0-50 Hz的带通滤波器对EEG数据进行处理，以消除噪声。
- 在信号分割中选择合适的滑动窗口大小，可以为训练提供足够的样本，也可以让片段有足够的数据点进行特征提取。提取每个电影片段(长度约为4 min)持续时间对应的脑电图信号，将每个会话的15个trail(电影片段)的数据分别分割为相同长度的10s样本，互不重叠。
- 提取5个频段，分别是delta: 1-3 Hz, theta: 4-7 Hz, alpha: 8-13 Hz, beta: 14-30 Hz, gamma: 31-50 Hz。

### 手工特征提取
微分熵(Differential entropy, DE)作为一种非线性熵度量，在脑电信号识别中表现出了出色的性能，尤其是对情绪脑电信号的识别。标机微分熵定义为:
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117104516.png#pic_center%20=400x)
其中X为随机变量，f (X)为X的概率密度函数。由于脑电图信号已被证明服从高斯分布N(µ，σ 2)，故微分熵可计算为:
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117104608.png#pic_center%20=400x)
同时，对于固定长度的脑电图片段，微分熵等价于某一频段的对数能量谱。我们分别从5个EEG频段(delta: 1-3 Hz, theta: 4-7 Hz, alpha: 8-13 Hz, beta: 14-30 Hz, gamma: 31-50 Hz)中提取DE特征，利用256点短时间傅里叶变换和1秒的非重叠汉宁窗口。因此，我们每秒得到N × 1 × C维DE特征，其中N表示EEG通道数，C为EEG带数，对于T秒长度的样本，我们得到N × T × C维DE特征。




### 深度卷积层

输入DE特征被馈送到深度卷积层，该层在模型中起着重要作用。

DCoT模型能够利用深度卷积层提取多频数据的完整信息。

然而，简单地将不同频率的DE特征在时间维度上进行拼接，使得DE特征序列时间不一致。这种操作会导致时序特征的丧失以及关节处的干扰。因此，引入卷积层来消除负面影响。卷积层的实现进一步融合了不同频率的DE特征，并生成了新的特征。随着卷积核沿时间维移动，我们得到了一个时间相干特征矩阵，如图2b所示。这样既考虑了同一数据点上不同频段之间的固有关系，又避免了这些不必要的外界干扰。

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117104709.png#pic_center%20=400x)

其次，利用卷积层不仅具有良好的局部特征捕获能力，而且可以从每个EEG通道中捕获频域特征，减少了后续编码器块的计算量。综上所述，深度卷积层应用提供了更有效的特征信息，可以提高情感识别结果。

同时，为保证各脑电信号通道特征的独立性，不适用正则卷积，因此采用深度卷积。

### Transformer
采用了Transformer的网络架构，该架构在自然语言处理（NLP）的翻译质量方面取得了优异的性能。与大多数竞争性神经序列转导模型一样，Transformer模块遵循encoder-decoder结构，使用stacked self-attention和point-wise全连接层。该模型将输入向量与三个不同的权重矩阵相乘，以获得查询向量（Q）、键向量（K）和值向量（V）。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815134134.png#pic_center%20=400x)

“缩放点积注意力”如图所示，该图计算了具有所有键的查询的点积，每个键除以√dk，并应用softmax函数以获得值的权重，如公式所示：

$$ Q=W_{Q} X $$
$$ K=W_{K} X $$
$$ V=W_{V} X $$

self-attention使用==点积==模型，输出向量可以写成公式。
$$ H=V \operatorname{softmax}\left(\frac{K^{\mathrm{T}} Q}{\sqrt{d_{k}}}\right) $$
之后$d_k$按比例缩放时，通过softmax函数获得每个输出通道上的注意力分数。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815134639.png#pic_center%20=400x)

“多头注意力”如图和公式所示：
$$MultiHead (\mathrm{Q}, \mathrm{K}, \mathrm{V})= Concat \left(\operatorname{head}_1, \ldots\right., head \left._{\mathrm{h}}\right) \mathrm{W}$$
$$Wherehead _{\mathrm{i}}= Attention (\mathrm{Q}, \mathrm{K}, \mathrm{V})$$
在本研究中，我们采用了h=8个平行注意层（所谓的8个注意头），并将Transformer的编码器部分单独嵌入到EEG分类中。如图所示
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20220815134745.png#pic_center%20=400x)

Transformer模块有两个子模块。第一个子模块包括一个多头注意力层，然后是一个规范化层。第二个子模块包括位置全连接前馈层，然后是归一化层。围绕两个子模块中的每一个子模块采用剩余连接。

### Positional Embedding
在自然语言处理领域，前人在Transformer模型中嵌入了Positional Embedding，以增加位置信息。对于EEG数据，我们探索了三类位置嵌入（PE）模块：相对位置编码、信道相关位置编码和学习位置编码。相对位置嵌入方法使用正弦和余弦函数来表示相对位置编码
$$ \begin{aligned} \mathrm{PE}_{(\text {pos }, 2 \mathrm{i})} &=\sin \left(\frac{\operatorname{pos}}{10000^{2 \mathrm{i} / \mathrm{d}}}\right) \\ P E_{(\operatorname{pos}, 2 \mathrm{i}+1)} &=\cos \left(\frac{\operatorname{pos}}{10000^{2 \mathrm{i} / \mathrm{d}}}\right) \end{aligned} $$

| 符号 | 含义 |
| -- | -- |
|pos|表示通道位置|
|i|表示时间点|
|d|表示向量的维数|

#### 原理
在电极矢量的每个位置，偶数和奇数时间点的PE分别由正弦函数和余弦函数描述，i是电极矢量中节点的索引除以2。我们对位置pos1和pos2进行了相对位置编码的内积，发现随着距离的增加，两个位置之间的相关性变小。

#### 应用
在通道相关Positional Embedding中，选择Cz电极作为中心电极，并计算其他电极与中心电极之间的余弦距离。Pcentral表示中心电极Cz的三维坐标，Pk是三维坐标的第k个位置。通过使用simk距离而不是公式中的pos来执行运算，所得矩阵是信道相关位置编码的位置编码矩阵。
$$ \operatorname{sim}_{\mathrm{k}}=\frac{\mathrm{P}_{\text {central }} \cdot \mathrm{P}_{\mathrm{k}}}{\left\|\mathrm{P}_{\text {central }}\right\|\left\|\mathrm{P}_{\mathrm{k}}\right\|} $$
在学习的位置嵌入方法中，我们将相同大小的可训练矩阵嵌入到输入中，并随机初始化嵌入矩阵。在模型训练过程中，通过学习不断更新位置编码矩阵的参数。

![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117105805.png#pic_center%20=400x)

### 模型
DCoT模型的概述如图所示。模型输入表示为X∈RN×T×C，其中N表示EEG channel的个数，T表示EEG sample的长度，C表示EEG frequency bands的个数
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117104325.png#pic_center%20=400x)

DCoT的基本组成部分包括深度卷积(DW-CONV)层、位置嵌入、可学习嵌入、变压器编码器和线性层。此外，变压器编码器由层归一化(LN)、多头自注意(MSA)层和前馈网络(FFN)组成

## 结果
### 不同频带下消极情绪和积极情绪的识别准确率
有DW-CONV层的模型在每个频段上的性能都比没有DW-CONV层的模型更好，这说明了引入DW-CONV层的效率。
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117111720.png#pic_center%20=400x)
### 不同被试准确率
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117111957.png#pic_center%20=400x)

### 模型比较
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117112012.png#pic_center%20=400x)
### 注意力得分
![Img](https://imgpool.protodrive.xyz/img/yank-note-picgo-img-20230117112106.png#pic_center%20=400x)
